{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import sqlite3\n",
      "import time\n",
      "from goose import Goose\n",
      "import pickle\n",
      "import pandas as pd\n",
      "import re\n",
      "from nltk import sent_tokenize, word_tokenize\n",
      "import csv\n",
      "import os"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def search_sents_for_google_locations(link_sql, csv_output):\n",
      "    \"\"\"\n",
      "    Main Function for Article Scrapper. Takes a sql script with all the urls to be searched\n",
      "    and searched the articles for mentions of any city states from Google's list online.\n",
      "    \n",
      "    CSV from the google page with the cities and states:\n",
      "    https://developers.google.com/adwords/api/docs/appendix/geotargeting?csw=1\n",
      "    \n",
      "    Returns a CSV with all the sentences and each mention of a city state in the article.\n",
      "    If no city states are mentioned, the 2nd column will be blank.    \n",
      "    \"\"\"\n",
      "    \n",
      "    all_article_urls = grab_available_urls(link_sql)\n",
      "    \n",
      "    google_cs_list = city_state_names()\n",
      "    \n",
      "    for url in all_article_urls:\n",
      "        try:\n",
      "            content = grab_article_content(url)\n",
      "            sents = tolkenize_sents_and_remove_tabs(content)\n",
      "            sents_with_locations = []\n",
      "            for next_sent in sents:\n",
      "                loc_tuple = find_google_cs(next_sent, google_cs_list)\n",
      "                for result in loc_tuple:\n",
      "                    sents_with_locations.append(result)\n",
      "            df = pd.DataFrame(sents_with_locations)\n",
      "            if not os.path.isfile(csv_output):\n",
      "                p_header = True\n",
      "            else:\n",
      "                p_header = False\n",
      "            df.to_csv(csv_output, sep=\",\", header=p_header, index=False, mode = 'a')\n",
      "        except:\n",
      "            with open(\"url_failed_entry_log.txt\", \"ab\") as f:\n",
      "                f.write(url + \" failed to log.\\n\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def tolkenize_sents_and_remove_tabs(content):\n",
      "    \"\"\"\n",
      "    Using the sent tokenize function from the nltk library,\n",
      "        returns the sentences.\n",
      "    The tabs have been removed and replaced with spaces.\n",
      "    \"\"\"\n",
      "    raw_sentences = sent_tokenize(content)\n",
      "    sents = [re.sub(\"\\t\", \" \", sent) for sent in raw_sentences]\n",
      "    return sents"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def grab_available_urls(article_sql):\n",
      "    \"\"\"\n",
      "    Returns a unique list of article urls from the \"TechScrapper\" sqlite database.\n",
      "    \"\"\"\n",
      "    conn = sqlite3.connect(\"TechScrapper.db\")\n",
      "    c = conn.cursor()\n",
      "    c.execute(article_sql)\n",
      "    urls = [row[0] for row in c.fetchall()]\n",
      "    conn.close()\n",
      "    return urls"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def grab_article_content(url):\n",
      "    \"\"\"\n",
      "    Using the Goose library, grabs the contents of the article.\n",
      "    The non-ascii characters have been replaced with '?'\n",
      "    The question marks are then removed from the sentences completely.\n",
      "    \"\"\"\n",
      "    g = Goose()\n",
      "    article = g.extract(url=url)\n",
      "    content = article.cleaned_text.encode(\"ascii\", \"replace\")\n",
      "    content = re.sub(\"\\?\", \" \", content)\n",
      "    return content"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def city_state_names():   \n",
      "    \"\"\"\n",
      "    Returns a dataframe with the unique list of locations from the google page.\n",
      "    \"\"\"\n",
      "    f = \"all_major_locations.csv\"\n",
      "    df = pd.DataFrame.from_csv(f)\n",
      "    names = df.Name.unique()\n",
      "    return names"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "SyntaxError",
       "evalue": "EOF while scanning triple-quoted string literal (<ipython-input-1-6b585f263d94>, line 6)",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-1-6b585f263d94>\"\u001b[1;36m, line \u001b[1;32m6\u001b[0m\n\u001b[1;33m    return names\u001b[0m\n\u001b[1;37m                \n^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m EOF while scanning triple-quoted string literal\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def find_google_cs(sent, google_cs_list):\n",
      "    \"\"\"\n",
      "    For each sentence, searches the list of google cities and states.\n",
      "    If a word is matches, then one record would be returned.\n",
      "    This record would include the sentence, the word, and the start and end location of the word itself.\n",
      "    If one word city, the start and end would equal each other.\n",
      "    If two word city, the end would be equal to one plus the start.\n",
      "    If no location is found, the sentence is returned with \"N/A\" as the location found,\n",
      "        with -1 returned for both the start and end location.\n",
      "        \n",
      "    \"\"\"\n",
      "    locations_found = []\n",
      "    for loc in google_cs_list:\n",
      "        all_finds = find_location_points(loc, sent)\n",
      "        for loc_f in all_finds:\n",
      "            part_sent = sent[:loc_f] #Remove the location found in the sentence\n",
      "            part_tokenized_sent = word_tokenize(part_sent) #Count the count of tolkenized words\n",
      "            first_point = len(part_tokenized_sent) #The First location will\n",
      "            tokenize_loc = word_tokenize(loc) #tolkenize the location\n",
      "            last_point = first_point + len(tokenize_loc) - 1 #The last point will be the first point plus the lenght of tolken (-1)\n",
      "            locations_found.append((sent, loc, first_point, last_point)) #Add this to the list\n",
      "    if len(locations_found) == 0:\n",
      "        return [(sent, \"N/A\", -1, -1)]\n",
      "    else:\n",
      "        return locations_found\n",
      "    \n",
      "def find_location_points(loc, sent):\n",
      "    re_ex = \"(?<!\\w)(\" + loc + \")(?!\\w)\"\n",
      "    all_finds = [m.start() for m in re.finditer(re_ex, sent)]\n",
      "    return all_finds"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def test():\n",
      "    sent_1 = \"I am the one who knocks in Utah.\"\n",
      "    sent_2 = \"I don't have any named states in here.\"\n",
      "    sent_3 = \"I am from San Francisco.\"\n",
      "    sent_4 = \"I am from Utah, and she too is from Utah.\"\n",
      "    sent_5 = \"Utah is my favorite\"\n",
      "    cities = [\"Omaha\", \"Utah\", \"Texas\", \"San Francisco\"]\n",
      "    assert find_google_cs(sent_1, cities)[0] == (\"I am the one who knocks in Utah.\", \"Utah\", 7, 7)\n",
      "    assert find_google_cs(sent_2, cities)[0] == (\"I don't have any named states in here.\", \"N/A\", -1, -1)\n",
      "    assert find_google_cs(sent_3, cities)[0] == (\"I am from San Francisco.\", \"San Francisco\" , 3, 4)\n",
      "    assert find_google_cs(sent_4, cities)[1] == (\"I am from Utah, and she too is from Utah.\", \"Utah\", 10, 10)\n",
      "    assert find_google_cs(sent_5, cities)[0] == (\"Utah is my favorite\", \"Utah\", 0, 0)\n",
      "    assert find_google_cs(sent_1, cities) == [((\"I am the one who knocks in Utah.\", \"Utah\", 7, 7))]\n",
      "    test_url = \"https://gigaom.com/2014/09/18/finally-meta-begins-shipping-its-augmented-reality-glasses-to-developers/\"\n",
      "    test_content = grab_article_content(test_url)\n",
      "    assert test_content[:5] == \"It s \"\n",
      "    \n",
      "    t_sent = \"Look at that \\t big old tab. There are lots of tabs! \\t\\t\\t It's an \\t epidemic!\"\n",
      "    nt_sent = \"No tabs here!\"\n",
      "    assert tolkenize_sents_and_remove_tabs(t_sent)[0] == 'Look at that   big old tab.'\n",
      "    assert tolkenize_sents_and_remove_tabs(t_sent)[1] == 'There are lots of tabs!'\n",
      "    assert tolkenize_sents_and_remove_tabs(nt_sent)[0] == nt_sent\n",
      "    \n",
      "    \n",
      "    location = \"Utah\"\n",
      "    assert find_location_points(location, sent_5) > 0\n",
      "    \n",
      "    \n",
      "    return \"Test Passed\"\n",
      "\n",
      "test()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 9,
       "text": [
        "'Test Passed'"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Main Function of the Script. Used to define how many articles would be scrapped\n",
      "#Also includes the time to grab the number of seconds that has passed from start to finish\n",
      "amount = '150'\n",
      "\n",
      "sql = \"SELECT ArticleUrl FROM Articles WHERE ID > 100 LIMIT \" + amount\n",
      "csv_output = \"test_output_\" + amount + \"_articles.csv\"\n",
      "now = time.time()\n",
      "search_sents_for_google_locations(sql, csv_output) \n",
      "end = time.time()\n",
      "print end - now"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "8953.91599989\n"
       ]
      }
     ],
     "prompt_number": 19
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "(end - now) / 60 / 60"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 20,
       "text": [
        "2.4871988888581593"
       ]
      }
     ],
     "prompt_number": 20
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}