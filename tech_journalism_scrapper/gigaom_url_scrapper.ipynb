{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "import numpy as np\n",
      "from pandas import DataFrame, Series\n",
      "from bs4 import BeautifulSoup\n",
      "import urllib2\n",
      "import datetime\n",
      "from dateutil.relativedelta import relativedelta"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 32
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def scrap_page(webpage, stop_date):\n",
      "    ##Opening the page and grabbing the html, as well as time stamping\n",
      "    response = urllib2.urlopen(webpage)\n",
      "    html = response.read()\n",
      "    time_collected = datetime.datetime.now()\n",
      "    response.close()\n",
      "    \n",
      "    ##Using BeautifulSoup to grab all articles\n",
      "    ##Giagom has articles in groups of divs\n",
      "    ##Two findAll's are necessary per page to grab the correct info\n",
      "    \n",
      "    articles = []\n",
      "    soup = BeautifulSoup(html)\n",
      "    art_divs = soup.findAll('div', id=lambda x: x and x.startswith('postloop-'))\n",
      "    articles = [ div.findAll('article', id = lambda x: x and x.startswith(\"post-\")) for div in art_divs]\n",
      "    ##articles has all articles in 2d-array. The below code flattens this into a 1d-array.\n",
      "    articles = [item for sublist in articles for item in sublist] \n",
      "\n",
      "    ##Searching article and grabbing basic elements necessary for scrapping\n",
      "    ##This section also has a logic statement looking for a stopping point\n",
      "    \n",
      "    scrapped_articles = []\n",
      "    continue_search = 1 #If 1, then continue. If 0, the scrapper\n",
      "    for a in articles:\n",
      "        a_values = {}\n",
      "        published_datetime = a.find(\"time\", { \"class\" : \"time published\" })['title']\n",
      "        published_datetime = datetime.datetime.strptime(published_datetime, \"%Y/%m/%d %H:%M:%S %p\")\n",
      "        if published_datetime <= stop_date:\n",
      "            continue_search = 0\n",
      "            break\n",
      "        a_values[\"published_datetime\"] = published_datetime.strftime(\"%Y-%m-%d %H:%m:%S\")\n",
      "        a_values[\"article_link\"] = a.find(\"h1\", {\"class\" : \"entry-title\"}).find(\"a\")['href']\n",
      "        a_values[\"website\"] = website\n",
      "        a_values[\"retrieved_datetime\"] = time_collected.strftime(\"%Y-%m-%d %H:%m:%S\")\n",
      "        scrapped_articles.append(a_values)\n",
      "    return scrapped_articles, continue_search"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 46
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "##Grabbing the website and adding functionality to go through multiple pages\n",
      "website = \"https://gigaom.com\"\n",
      "next_page = \"/page/\"\n",
      "next_page_int = 2\n",
      "##Adding functionality to stop searching on or befor a certain date\n",
      "##For now, the statement will stop as soon as it finds and article from one month ago (whole months, not 30 days)\n",
      "stop_date = time_collected-relativedelta(months=+1)\n",
      "\n",
      "#Loop to Search for Articles\n",
      "continue_search = 1\n",
      "next_page = website #Creating a new variable for searching pages\n",
      "while continue_search == 1 or next_page_int <= 500: #Forcing Termination after 500 pages\n",
      "    scrapped_articles, continue_search = scrap_page(webpage, stop_date)\n",
      "    break\n",
      "    webpage=\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 47
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "scrapped_articles[2]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 48,
       "text": [
        "{'article_link': 'https://gigaom.com/2014/09/13/13-google-chrome-extensions-you-should-know-about/',\n",
        " 'published_datetime': '2014-09-13 10:09:39',\n",
        " 'retrieved_datetime': '2014-09-13 16:09:42',\n",
        " 'website': 'https://gigaom.com'}"
       ]
      }
     ],
     "prompt_number": 48
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}