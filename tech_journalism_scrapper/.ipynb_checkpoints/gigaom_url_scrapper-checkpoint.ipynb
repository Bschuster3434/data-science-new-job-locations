{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "import numpy as np\n",
      "from pandas import DataFrame, Series\n",
      "from bs4 import BeautifulSoup\n",
      "import urllib2\n",
      "import datetime\n",
      "from dateutil.relativedelta import relativedelta\n",
      "import sqlite3\n",
      "import time"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 54
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def scrap_page(webpage, stop_date):\n",
      "    ##Opening the page and grabbing the html, as well as time stamping\n",
      "    response = urllib2.urlopen(webpage)\n",
      "    html = response.read()\n",
      "    time_collected = datetime.datetime.now()\n",
      "    response.close()\n",
      "    \n",
      "    ##Using BeautifulSoup to grab all articles\n",
      "    ##Giagom has articles in groups of divs\n",
      "    ##Two findAll's are necessary per page to grab the correct info\n",
      "    \n",
      "    articles = []\n",
      "    soup = BeautifulSoup(html)\n",
      "    art_divs = soup.findAll('div', id=lambda x: x and x.startswith('postloop-'))\n",
      "    articles = [ div.findAll('article', id = lambda x: x and x.startswith(\"post-\")) for div in art_divs]\n",
      "    ##articles has all articles in 2d-array. The below code flattens this into a 1d-array.\n",
      "    articles = [item for sublist in articles for item in sublist] \n",
      "\n",
      "    ##Searching article and grabbing basic elements necessary for scrapping\n",
      "    ##This section also has a logic statement looking for a stopping point\n",
      "    \n",
      "    scrapped_articles = []\n",
      "    continue_search = 1 #If 1, then continue. If 0, the scrapper\n",
      "    for a in articles:\n",
      "        a_values = {}\n",
      "        published_datetime = a.find(\"time\", { \"class\" : \"time published\" })['title']\n",
      "        published_datetime = datetime.datetime.strptime(published_datetime, \"%Y/%m/%d %H:%M:%S %p\")\n",
      "        if published_datetime <= stop_date:\n",
      "            continue_search = 0\n",
      "            break\n",
      "        a_values[\"published_datetime\"] = published_datetime.strftime(\"%Y-%m-%d %H:%m:%S\")\n",
      "        a_values[\"article_link\"] = a.find(\"h1\", {\"class\" : \"entry-title\"}).find(\"a\")['href']\n",
      "        scrapped_articles.append(a_values)\n",
      "    return scrapped_articles, continue_search"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 55
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "##Grabbing the website and adding functionality to go through multiple pages\n",
      "website = \"https://gigaom.com\"\n",
      "page_end = \"/page/\"\n",
      "next_page_int = 12\n",
      "##Adding functionality to stop searching on or befor a certain date\n",
      "##For now, the statement will stop as soon as it finds and article from one month ago (whole months, not 30 days)\n",
      "stop_date = time_collected-relativedelta(months=+1)\n",
      "\n",
      "##Opening connection to db\n",
      "conn = sqlite3.connect(\"TechScrapper.db\")\n",
      "c = conn.cursor()\n",
      "\n",
      "#Loop to Search for Articles\n",
      "continue_search = 1\n",
      "next_page = website #Creating a new variable for searching pages\n",
      "while continue_search == 1 and next_page_int <= 100: #Forcing Termination after a certain number of pages\n",
      "    scrapped_articles, continue_search = scrap_page(next_page, stop_date)\n",
      "    ##Insert into db_table\n",
      "    for article in scrapped_articles:\n",
      "        a_url = article[\"article_link\"]\n",
      "        a_published = article[\"published_datetime\"]\n",
      "        a_retrieved = time_collected.strftime(\"%Y-%m-%d %H:%m:%S\")\n",
      "        c.execute(\"\"\"INSERT INTO Articles VALUES(?,?,?,?,?)\"\"\", \n",
      "              (None, a_url, a_published, website, a_retrieved))\n",
      "        conn.commit()\n",
      "    time.sleep(.2)\n",
      "    print \"Completed Page \" + str(next_page_int)\n",
      "    next_page_int += 1\n",
      "    next_page = website + page_end + str(next_page_int)\n",
      "\n",
      "conn.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Completed Page 12\n",
        "Completed Page 13"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Completed Page 14"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Completed Page 15"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Completed Page 16"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Completed Page 17"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Completed Page 18"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Completed Page 19"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Completed Page 20"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 78
    }
   ],
   "metadata": {}
  }
 ]
}